image:
  repository: vllm/vllm-openai
  tag: v0.8.5
# https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
priority: -100
model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  # name: "ibm-granite/granite-4.0-tiny-preview"
  # name: "nlpcloud/instruct-gpt-j-fp16"
  # name: "mistralai/Mistral-7B-Instruct-v0.2"
  # name: "meta-llama/Llama-3.2-3B-Instruct"
service:
  type: ClusterIP
resources:
  # just one gpu per node for now
  # down the line we will try to figure out improvements
  requests:
    cpu: "400m"
    memory: "2G"
    nvidia.com/gpu: 1
  limits:
    cpu: "700m"
    memory: "4G"
    nvidia.com/gpu: 1
