image:
  repository: vllm/vllm-openai
  tag: latest
# https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
priority: -100
model:
  name: "mistralai/Mistral-7B-Instruct-v0.2"
service:
  type: ClusterIP
resources:
  # just one gpu per node for now
  # down the line we will try to figure out improvements
  limits:
    nvidia.com/gpu: 1
  requests:
    nvidia.com/gpu: 1
