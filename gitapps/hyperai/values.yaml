image:
  repository: vllm/vllm-openai
  tag: latest
# https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
priority: -100
model:
  name: "nlpcloud/instruct-gpt-j-fp16"
  # name: "mistralai/Mistral-7B-Instruct-v0.2"
  # name: "meta-llama/Llama-3.2-3B-Instruct"
service:
  type: ClusterIP
resources:
  # just one gpu per node for now
  # down the line we will try to figure out improvements
  requests:
    cpu: "200m"
    memory: "128Mi"
    nvidia.com/gpu: 1
  limits:
    cpu: "400m"
    memory: "1024Mi"
    nvidia.com/gpu: 1
